# 概览
2020年正式参加工作了，生活和技术上都成长了不少，这篇是技术总结，只谈工作和技术相关的部分了。


简而言之，今年做的事儿有如下几项：

- 一个交接过来的Node.js项目的业务开发与维护
- 上述项目的工程化CI改造、项目优化
- 一个DDD项目的开发
- 尝试面试别人



学到的技术：

- K8s，通过读《Kubernetes in Action》了解了K8s的基本组件和概念，延伸下来了解了一些云原生的概念
- Kafka，项目中简单实践了下，引入会带来一定的开发复杂度，但对于项目整体解耦还是很有效的
- DDD，领域驱动设计，在开发一个项目时完整实践了下，效果很好，对降低开发中的心智负担效果极好，而且可以很好地按照「聚合」的概念组织起开发设计到的相关资源，例如API设计、数据库、文件存储等，但是也确实会引入一定的开发复杂度，整体而言利大于弊。简单项目确实没必要严格实现 DDD 这一套，最大的复杂度来源于 Repo 的设计，但是 DDD 思想还是可以广泛应用于很多项目的
- Saga，通过《Microservice Patterns》学习了下这个分布式事务的解决方案，对于事务和性能要求比较苛刻的场景下确实有需求，但是带来的复杂度也很高，需要慎重考虑，目前还未尝试使用



# 运营平台
2020年在工作占用时间最多的一件事就是运营平台开发。接手过来的时候是一个整体的外包项目，当时已经是第二期了，中间还换过一次开发商。所以代码里混杂着一期外包的设计、二期外包的代码、组里自己修改的部分代码。整个项目就是DDD提到的大泥球（Big Mud），光是梳理清楚每个文件夹底下的东西是啥，就用了两周。后来发现，虽然项目拆成了七个子模块，但是模块与模块间的耦合十分严重，没有很大的拆分空间，简单来说只能拆成四部分：前端、接入层、核心业务服务、用户服务。这四个模块也是直接部署到了同一个Pod中，简直是把云原生十二要素都抛之脑后了，容器也被当成了虚拟机用，整体项目思想停留在上个时代。


由于以上这些缘故，这半年的工作除了需求开发外，主要精力都投入在如何对平台的优化上了。


# K8s部署改造
不详细展开了，主要是如下几点：


- 部分配置迁移至环境变量
- 服务无状态化
- 使用 `curl` 作为 `liveness probe` 和 `readiness probe` ，杜绝异常501问题，支持无损发布



## CI优化
大泥球模式最严重的影响，就是CI时间的爆炸性增长。随着服务中引入的依赖越来越多，业务代码越来越复杂，编译CI从最开始的10m耗时上涨到半小时，这导致开发和部署效率十分低下，在生产环境出现线上bug时，即使是很小的bug，也得至少先等个半小时才能修复。今年五月份，大家被这个问题搞得很难受，我看了下构建脚本，发现构建慢主要是由如下两点导致的：


1. 同步执行不同项目的构建命令
1. 巨型单体项目构建产物从CI拷贝到容器镜像中，再从CI发送到CD平台，IO消耗了大量的时间



针对如上第一点，可以通过并行化提速，因为不同构建任务间是不相关，没有依赖关系的。简单处理了之后，构建从半小时降低到20分钟，有效果，但不显著。


针对第二点，我将项目拆分为了两部分，分别创建各自的CI进行构建和部署：


- 前端 / 接入层
- 核心业务 / 用户服务



这一步大大降低了每个项目的构建产物的大小，由此加速了CI过程，现在每个项目可以在六分钟内完成，并且前后端项目在部署上也独立了。

完成了上述拆分后，我又对CI本身进行了一些改进，优化了整个项目的CI方案。现在，开发者代码编写完成，只要在希望上线的位置打好Tag，便会触发上述CI流水线，CI便会自动完成项目的构建、前端资源文件上传COS、部署到测试环境、部署到线上环境等重复操作，这也是云原生所推崇的。完成这个CI优化后，我是真切地感受到了CI对于工作效率的大幅度提升，也意识到了Shell语言作为最基础的脚本语言，在Linux环境下无与伦比的重要性。


## 前端资源静态化优化
这个项目在一开始是把前端用到的所有文件放在容器中的，这样会把所有前端压力打到我们自己的服务上，比较占用带宽，但我们用户量不大，这个影响并没有什么，致命的是另外一个问题。


现代前端构建工具都会将前端文件构建后，按照依赖和子页面分解为多个子文件，每个子文件使用ContentHash算法重命名，这是为了及时更新相关的CDN缓存。但是，反向代理会用 `roundrobin` 算法从后端加载资源进行负载均衡。这就出现了如下图的问题：


![image.png](https://cdn.nlark.com/yuque/0/2021/png/657413/1609754301215-5cc5bce7-5b59-4598-8d81-252a1533b923.png#align=left&display=inline&height=304&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1216&originWidth=2162&size=834745&status=done&style=none&width=541)


前端加载页面是先读取 `index.html` ，然后依次从中加载js、css文件，我们的业务比较繁杂，这些js和css文件可能多达上百个，此时这些加载请求经过 `roundrobin` 算法后打到了后台不同版本的服务器上，但是 `abcd.js` 这个文件存在于1.1.0的服务中，如果这个请求打到了1.0.0的服务上，就会返回404，导致页面加载失败，用户看来也就是白屏了。


最简单的方案是蓝绿发布，但是，目前最新的K8s 1.19版本也尚未原生支持。所以只能另寻他法。


我们的解决方案也很简单，我们在服务中只保留 `index.html` ，将不同版本 `index.html` 依赖的 `abcd.js` 和 `wxyz.js` 统一放到COS上，这样可以保证不同的js文件可以同时存在，不会随着滚动更新消失。最终用户能看到的页面版本取决于用户拿到的 `index.html` 版本。静态资源的路径前缀可以通过webpack设置，文件上传至COS已经内置于上述CI流水线中。


![image.png](https://cdn.nlark.com/yuque/0/2021/png/657413/1609759869022-469b8c9d-bacd-4cb7-a6ce-bf297b49563a.png#align=left&display=inline&height=471&margin=%5Bobject%20Object%5D&name=image.png&originHeight=2702&originWidth=3198&size=2079592&status=done&style=none&width=558)
静态资源COS化以后不仅解决了404的问题，平台整体使用体验也得益于COS带来的响应速度提升而有所改善。未来还可以通过引入CDN，进一步提升平台前端加载速度，改善用户体验。


